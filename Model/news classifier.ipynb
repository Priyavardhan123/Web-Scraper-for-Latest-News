{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resident-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.layers import Embedding\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-zambia",
   "metadata": {},
   "source": [
    "<h1> Assesing Data Quality </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "certified-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json('News_Category_Dataset_v2.json', lines=True)\n",
    "dataset.drop(['authors','link','date'], axis=1, inplace=True)\n",
    "dataset.head(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-singer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rental-retailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique Categories : \".format(dataset['category'].nunique()))\n",
    "dataset['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.pie(dataset['category'].value_counts().values, labels=dataset['category'].value_counts().index, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-sentence",
   "metadata": {},
   "source": [
    "<h3> Grouping the Categories</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "criminal-discrimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = dataset['category'].value_counts().index\n",
    "\n",
    "def groups(grouplist, name):\n",
    "    for element in categories:\n",
    "        if element in grouplist:\n",
    "            dataset.loc[dataset['category']==element, 'category']=name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acting-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups(grouplist=['WELLNESS', 'HEALTHY LIVING' , 'HOME & LIVING', 'STYLE & BEAUTY', 'STYLE'] , name='LIFESTYLE AND WELLNESS')\n",
    "\n",
    "groups(grouplist=['PARENTING', 'PARENTS', 'EDUCATION', 'COLLEGE'] , name='PARENTING AND EDUCATION')\n",
    "\n",
    "groups(grouplist=['SPORTS', 'ENTERTAINMENT', 'COMEDY', 'WEIRD NEWS', 'ARTS'] , name='SPORTS AND ENTERTAINMENT')\n",
    "\n",
    "groups(grouplist=['TRAVEL', 'ARTS & CULTURE','CULTURE & ARTS','FOOD & DRINK', 'TASTE'] , name='TRAVEL-TOURISM & ART-CULTURE')\n",
    "\n",
    "groups(grouplist=['WOMEN','QUEER VOICES', 'LATINO VOICES', 'BLACK VOICES'] , name='EMPOWERED VOICES')\n",
    "\n",
    "groups(grouplist=['BUSINESS' ,  'MONEY'] , name='BUSINESS-MONEY')\n",
    "\n",
    "groups(grouplist=['THE WORLDPOST' , 'WORLDPOST' , 'WORLD NEWS'] , name='WORLDNEWS')\n",
    "\n",
    "groups(grouplist=['ENVIRONMENT' ,'GREEN'] , name='ENVIRONMENT')\n",
    "\n",
    "groups(grouplist=['TECH', 'SCIENCE'] , name='SCIENCE AND TECH')\n",
    "\n",
    "groups(grouplist=['FIFTY' , 'IMPACT' ,'GOOD NEWS','CRIME'] , name='GENERAL')\n",
    "groups(grouplist=['WEDDINGS', 'DIVORCE',  'RELIGION','MEDIA'] , name='MISC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The revised Categories are : \".format(dataset['category'].nunique()))\n",
    "dataset['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "plt.pie(dataset['category'].value_counts().values, labels=dataset['category'].value_counts().index, autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-christopher",
   "metadata": {},
   "source": [
    "<h3> Removing empty values and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "international-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.copy() # creating a copy of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()  # total duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-carol",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated(subset=['short_description', 'headline']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['short_description', 'headline'], keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-junction",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['headline'] == \"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-representation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['headline'] == \"\", 'headline'] =np.nan\n",
    "df.dropna(subset=['headline'], inplace=True)\n",
    "print(len(df[df['headline'] == \"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df[df['short_description'] == \"\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concrete-cabinet",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['short_description'] == \"\", 'short_description'] = np.nan\n",
    "df.dropna(subset=['short_description'], inplace=True)\n",
    "print(len(df[df['short_description'] == \"\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-courtesy",
   "metadata": {},
   "source": [
    "<h3> Data Tidying </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chronic-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "df =shuffle(df)\n",
    "df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-surveillance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['desc'] = df['headline'].astype(str)+\"-\"+df['short_description']\n",
    "df.drop(columns =['headline', 'short_description'], axis=1, inplace=True)\n",
    "df.astype(str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cooked-oriental",
   "metadata": {},
   "source": [
    "<h2> Tokenizing and Padding </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y =df['desc'], df['category']\n",
    "\n",
    "#Dividing our data as the following:\n",
    "# Train data : 80%\n",
    "# Test data : 10%\n",
    "# Validation data : 10%\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,Y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-disclosure",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size =20000\n",
    "max_length =150\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok=\"<OOV>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "word_index=tokenizer.word_index\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(X_train, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
    "y_train = np.asarray(y_train)\n",
    "y_train = pd.get_dummies(y_train)\n",
    "\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "X_val = pad_sequences(X_val, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
    "y_val = np.asarray(y_val)\n",
    "y_val = pd.get_dummies(y_val)\n",
    "\n",
    "train_set = np.array(X_train)\n",
    "val_set = np.array(X_val)\n",
    "\n",
    "train_label = np.array(y_train)\n",
    "val_label = np.array(y_val)\n",
    "\n",
    "y_test = pd.get_dummies(y_test)\n",
    "y_test = np.asarray(y_test)\n",
    "y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(train_set.shape)\n",
    "print(train_label.shape)\n",
    "print(val_set.shape)\n",
    "print(val_label.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-woman",
   "metadata": {},
   "source": [
    "<h2> Embedding Matrix for our Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(tokenizer.word_index.items()) + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(\"glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit = 1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep = \" \")\n",
    "        embeddings_index[word] = coefs\n",
    "        \n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automated-oasis",
   "metadata": {},
   "source": [
    "<h6> Preparing Embedding Matrix </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-absolute",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "\n",
    "print(\"Converted %d words (%d misses)\" % (hits,misses))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-length",
   "metadata": {},
   "source": [
    "<h2> Training our Model </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-journalism",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 3, min_delta = 0.0001)\n",
    "tf.keras.backend.clear_session()\n",
    "embed_size = 100\n",
    "model = keras.models.Sequential([\n",
    "    Embedding(num_tokens, embedding_dim, embeddings_initializer = keras.initializers.Constant(embedding_matrix), mask_zero = True, input_shape = [None], trainable =False),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(256, dropout = 0.4)),\n",
    "    keras.layers.Dense(12, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-mount",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam(learning_rate = 0.001)\n",
    "model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_set, train_label, batch_size = 32, steps_per_epoch = len(X_train) // 32, validation_data = (val_set, val_label), validation_steps = len(val_set) // 32 , epochs =20, callbacks = early_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-clear",
   "metadata": {},
   "source": [
    "<h2> Evaluating and Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "round-garlic",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = dataset['category'].value_counts().index\n",
    "\n",
    "def prediction(inference_data) :\n",
    "    X = tokenizer.texts_to_sequences(inference_data)\n",
    "    X = pad_sequences(X, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n",
    "    pred = model.predict(X)\n",
    "    pred_value = tf.argmax(pred, axis = 1).numpy()\n",
    "    return pred_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = prediction(X_test)\n",
    "print(classification_report(np.asarray(y_test), np.asarray(y_pred)))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-surge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
